# Self-Diagnosing Neural Models: Uncertainty Quantification & Unsupervised Confidence Estimation

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)

Author: Sourav Roy  
Email: royxlead@proton.me  
Date: October 2025

---

## Project Overview

This repository contains a complete implementation and evaluation suite for "Self-Diagnosing Neural Models": uncertainty quantification (UQ) methods and a novel unsupervised confidence metric that estimates model confidence without using labels. The project includes:

- Implementations of multiple UQ methods: Baseline (MSP), Monte Carlo Dropout (MC Dropout), Evidential Deep Learning (EDL), and Deep Ensembles.
- A novel unsupervised confidence metric combining prediction consistency across augmentations, entropy, feature-space dispersion, and softmax temperature analysis.
- Comprehensive training, evaluation, visualization, and ablation study code.
- Pretrained model checkpoints and a final report with summarized results.

The experiments in this repo use CIFAR-10 as the in-distribution (ID) dataset and CIFAR-100 as out-of-distribution (OOD) for detection experiments. The code is organized primarily inside the Jupyter notebook `self_diagnosing_neural_models_python.ipynb` which contains the full pipeline and helper classes.

---

## Repository Structure (relevant files)

- `self_diagnosing_neural_models_python.ipynb` - Main notebook with code, training pipeline, evaluation, visualizations, and results.
- `final_report.txt` - Human-readable final report generated by the pipeline (also created manually in the workspace). Contains a concise summary of results and conclusions.
- `runs/meta_1760506599.json` - Run metadata including timestamp, seed, device, args used.
- `checkpoints/` - Directory containing saved model checkpoints used during experiments:
  - `baseline_best.pth`, `baseline_latest.pt`
  - `evidential_best.pth`, `evidential_latest.pt`
  - `mcdropout_best.pth`, `mcdropout_latest.pt`
- `ensemble_model_0.pth` ... `ensemble_model_6.pth` - Saved ensemble member weights (if ensemble training was run).

---

## Key Results (from final report)

- Dataset: CIFAR-10 (ID) vs CIFAR-100 (OOD)
- Training epochs: 100

Model performance summary (extracted):

- Baseline: Accuracy 91.14%, ECE 0.0323, OOD AUROC 0.8549, Time per sample ≈ 0.25 ms
- MC Dropout: Accuracy 90.78%, ECE 0.0097, OOD AUROC 0.8531, Time per sample ≈ 5.40 ms
- Evidential: Accuracy 91.66%, ECE 0.0742, OOD AUROC 0.8440, Time per sample ≈ 0.25 ms
- Ensemble: Accuracy 91.14%, ECE 0.0323, OOD AUROC 0.8549, Time per sample ≈ 0.85 ms

Unsupervised metric performance (Spearman correlation with true errors and Q4 accuracy):

- Baseline: Spearman 0.4115, Q4 Accuracy 99.68%
- MC Dropout: Spearman 0.4221, Q4 Accuracy 99.92%
- Evidential: Spearman 0.3708, Q4 Accuracy 98.60%
- Ensemble: Spearman 0.4127, Q4 Accuracy 99.68%

Key takeaways:

- Best accuracy: Evidential (91.66%)
- Best calibration (lowest ECE): MC Dropout (0.0097)
- Best OOD detection (AUROC): Baseline / Ensemble (0.8549)
- Best unsupervised metric correlation: MC Dropout (|ρ| = 0.4221)

---

## Installation

The notebook lists recommended versions for core libraries. Below is a minimal set of dependencies. Prefer creating a virtual environment or conda environment.

Python (tested with): 3.10+  
PyTorch: 2.8.0+cu126 (noted in run metadata)  

Minimum recommended pip install (adjust CUDA/cuDNN versions for your machine):

```powershell
# Create venv (optional)
python -m venv .venv; .\.venv\Scripts\Activate.ps1

# Install packages (adjust torch install command for your CUDA)
pip install numpy==1.26.4 scipy==1.13.1 scikit-learn==1.5.0 matplotlib==3.9.0 seaborn==0.13.2 tqdm==4.66.4 tensorboard==2.15.1
# Install torch+torchvision according to your CUDA. Example CPU-only or CUDA 11/12 wheel from pytorch.org
pip install torch torchvision
```

Note: The notebook includes commented pip install lines with specific versions. Use those if you want an exact reproduction environment.

---

## Quick Usage

Open the `self_diagnosing_neural_models_python.ipynb` notebook in Jupyter or VS Code and run cells sequentially. The notebook contains a `main_pipeline(...)` function that orchestrates dataset loading, training, evaluation, visualizations, and report generation.

High-level example (inside notebook or script):

```python
# From within the notebook or a Python script
from self_diagnosing_neural_models_python import main_pipeline

models, results, unsupervised_results, evaluator = main_pipeline(
    train_models=True,    # Set to False to load existing checkpoints
    num_epochs=100,
    run_ablations=True,
    id_dataset='cifar10',
    ood_dataset='cifar100',
    batch_size=128
)
```

Command-line execution (script mode from the notebook cell or when converted to .py):

```powershell
python self_diagnosing_neural_models_python.ipynb  # Not directly runnable as script; convert the notebook to .py first or use the CLI block in the notebook
```

Or, use the CLI section inside the notebook (there is an argparse block) when running the converted script.

---

## Checkpoints and How to Use Them

The `checkpoints/` folder contains saved weights for baseline, evidential, and MC dropout models. There are also `ensemble_model_*.pth` files at the repo root for ensemble members. Example usage:

```python
import torch
from notebook_models import BaselineModel

model = BaselineModel(num_classes=10)
model.load_state_dict(torch.load('checkpoints/baseline_best.pth'))
model.eval()
```

If you're using GPU, ensure the device mapping when loading:

```python
map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
state = torch.load('checkpoints/baseline_best.pth', map_location=map_location)
model.load_state_dict(state)
```

For ensembles, load each `ensemble_model_i.pth` into a `BaselineModel` instance and wrap them using the `DeepEnsemble` class.

---

## Reproducing the Results

1. Install dependencies and ensure PyTorch is configured for your CUDA or CPU.
2. Open the notebook and run the cells from the top. The notebook sets seeds for reproducibility.
3. If you want a fast smoke test without downloading datasets, run the notebook's smoke-test flag (the CLI contains a `--smoke-test` option).

Tips:
- To run a fast debug: set the environment variable `FAST_DEBUG_SUBSET` to a small number or use the `--fast-debug` CLI flag in the notebook's CLI block.
- Training full experiments requires GPUs and several hours depending on hardware—consider using fewer epochs or subset sizes for quick iteration.

---

## Visualizations and Outputs

The pipeline saves many publication-ready PNG visualizations into the working directory:

- Training curves: `<Model>_training_curves.png`
- Reliability diagrams: `<MODEL>_reliability_diagram.png`
- Confidence distributions: `confidence_distributions.png`
- Comparison metrics: `comparison_metrics.png`
- ROC curves for OOD detection: `ood_roc_curves.png`
- Unsupervised metric analysis: `<model>_unsupervised_analysis.png`
- Ablation study plots: `ablation_*.png`

The final report is saved to `final_report.txt` and a pickle of results is saved to `evaluation_results.pkl`.

---

## Notebook Highlights (Code Components)

- `DatasetManager` - loads ID and OOD datasets, supports CIFAR10, CIFAR100, MNIST, SVHN, and ImageFolder.
- `BaselineModel`, `MCDropoutModel`, `EvidentialModel` - model classes with forward/prediction utilities.
- `UnsupervisedConfidenceMetric` - main thesis contribution; computes a combined unsupervised confidence score.
- `Trainer`, `DeepEnsemble`, `ComprehensiveEvaluator`, `Visualizer`, `AblationStudies` - utilities to train, evaluate, visualize, and run ablations.
- `main_pipeline(...)` - orchestrates full experiment execution and report generation.

---

## Reproducibility & Metadata

The `runs/` directory contains metadata JSON files (e.g., `meta_1760506599.json`) with configuration details. Example contents include seed, device, torch version, and CLI args.

---

## Contact and Citation

If you use this work, please cite the project and contact the author for questions or collaboration: Sourav Roy <royxlead@gmail.com>.

---

## Next Steps & Suggestions

Optional improvements you might want to add to the repository:

- Add a `requirements.txt` or `environment.yml` for exact dependency pinning.
- Convert the notebook into a clean Python package/module for easier CLI execution and CI testing.
- Add unit tests (smoke tests) and a GitHub Actions workflow for CI.
- Include generated PNGs and link them in the README with relative paths.
- Add a sample script `run_experiment.py` that imports the pipeline functions and provides a clearer script-based CLI.

---


## License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

---

Generated on 2025-10-15 by analyzing the notebook and final report in this workspace.
